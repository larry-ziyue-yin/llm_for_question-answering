{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAGAS Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install required packages.\n",
    "\n",
    "Please offer any feedback if there are additional required packages so that I can make any updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install ragas\n",
    "%pip install langchain-together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct the evaluation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ragas(dataset_for_eval):\n",
    "    # Load the dataset.\n",
    "    import json\n",
    "    from datasets import Dataset\n",
    "    with open(dataset_for_eval, 'r', encoding='utf-8') as file:\n",
    "        data_raw = json.load(file)\n",
    "    dataset = Dataset.from_dict(data_raw)\n",
    "\n",
    "    # Deploy the model from LangChain Together AI.\n",
    "    from langchain_together import Together\n",
    "    from langchain_together.embeddings import TogetherEmbeddings\n",
    "    import os\n",
    "    os.environ[\"TOGETHER_API_KEY\"] = \"ceeef42d7bed8f09c9ca2b2a5e81d55b5389398566cd20698d12238c79a68abb\"\n",
    "    together_key = \"ceeef42d7bed8f09c9ca2b2a5e81d55b5389398566cd20698d12238c79a68abb\"\n",
    "    embeddings = TogetherEmbeddings(model=\"togethercomputer/m2-bert-80M-8k-retrieval\")\n",
    "    together_completion = Together(\n",
    "        model=\"NousResearch/Nous-Hermes-2-Mixtral-8x7B-SFT\",\n",
    "        temperature=0.7,\n",
    "        max_tokens=4000,\n",
    "        top_k=1,\n",
    "        together_api_key=together_key\n",
    "    )\n",
    "\n",
    "    # Conduct RAGAS through four main metrics.\n",
    "    from ragas import evaluate\n",
    "    from ragas.metrics import faithfulness, answer_relevancy, context_precision, context_recall\n",
    "    score = evaluate(dataset, metrics=[faithfulness, answer_relevancy, context_precision, context_recall], llm=together_completion, embeddings=embeddings, raise_exceptions=False)\n",
    "    score_df = score.to_pandas()\n",
    "    # print(score_df)\n",
    "    score_df.to_csv(dataset_for_eval, index=False)\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the evaluation metrics.\n",
    "\n",
    "Note that the `dataset_for_eval.json` should be replaced with the dataset generated from `new_rag.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ragas(\"dataset_for_eval.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ragas",
   "language": "python",
   "name": "ragas"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
